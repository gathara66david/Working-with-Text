{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gutenberg_cleaner\n",
    "from urllib.request import urlopen \n",
    "from gutenberg_cleaner import simple_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.gutenberg.org/files/2600/2600-0.txt'  # Verified working\n",
    "wnp = urlopen('https://www.gutenberg.org/files/2600/2600-0.txt').read().decode('utf-8')\n",
    "wnp = simple_cleaner(wnp)\n",
    "wnp[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = wnp.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "wnp = re.sub(r'\\s+', ' ', wnp).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "wnp = wnp.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = re.sub(r'\\w+@\\w+\\.\\w+', '', wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words \n",
    "non_stopwords = [] \n",
    "for word in wnp.split():  \n",
    "        if word not in _stop_words.ENGLISH_STOP_WORDS: \n",
    "                non_stopwords.append(word) \n",
    "cleaned_text = ' '.join(non_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer \n",
    "stemmer = SnowballStemmer('english') \n",
    "stemmed_words = [] \n",
    "for word in cleaned_text.split(): \n",
    "    stemmed_words.append(stemmer.stem(word)) \n",
    "stemmed_text = ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "def clean_text_spacy(text): \n",
    "    processed_text = spacy_en_model(text) \n",
    "    lemmas = [w.lemma_ if w.lemma_ != '-PRON-' \n",
    "              else w.lower_ for w in processed_text \n",
    "              if w.is_alpha and not w.is_stop] \n",
    "    return ' '.join(lemmas).lower() \n",
    "wnp = urlopen('https://www.gutenberg.org/files/2600/2600-0.txt').read().decode('utf-8') \n",
    "wnp = simple_cleaner(wnp) \n",
    "lemmatized_text = clean_text_spacy(wnp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en_model = spacy.load('en_core_web_sm') \n",
    "spacy_en_model.pipe_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_lg', disable=['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "processed_text = spacy_en_model(wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in processed_text[:10]: \n",
    "    print(word.text, word.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups \n",
    "newsgroups_train = fetch_20newsgroups(remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "ng_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import swifter \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_df['text'] = ng_train_df['text'].swifter.apply(clean_text_spacy) \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "ng_train_tfidf = tfidf_vectorizer.fit_transform \n",
    "(ng_train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.nlp import setup, plot_model \n",
    "nlp_setup = setup(newsgroups_train['data'], custom_stopwords= ['ax', 'edu', 'com', 'write']) \n",
    "plot_model(model=None, plot='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist \n",
    "fd = FreqDist(lemmatized_text.split()) \n",
    "fd.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    "fd_bg = FreqDist(map(' '.join, \n",
    "bigrams(lemmatized_text.split()))) \n",
    "fd_bg.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "idx_to_word = {v: k for k, v in \n",
    "tfidf_vectorizer.vocabulary_.items()} \n",
    "num_words = 20 \n",
    "tfidf_sum = np.asarray(ng_train_tfidf.sum(axis=0)).flatten() \n",
    "sorted_idx = tfidf_sum.argsort()[::-1] \n",
    "tfidf_sum = tfidf_sum[sorted_idx] \n",
    "xticks = range(num_words) \n",
    "plt.bar(xticks, tfidf_sum[:num_words]) \n",
    "plt.xticks(xticks, \n",
    "           [idx_to_word[i] for i in sorted_idx[:num_words]], \n",
    "           rotation=90) \n",
    "plt.xlabel('word') \n",
    "plt.ylabel('TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.histplot(ng_train_df['text'].apply(lambda x: len(x))) \n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "def get_top_grams(docs, n=2): \n",
    "    v = CountVectorizer(ngram_range=(n, n)) \n",
    "    grams = v.fit_transform(docs) \n",
    "    gram_sum = np.array(np.sum(grams, axis=0)).flatten() \n",
    "    gram_dict = {i: v for v, i in v.vocabulary_.items()} \n",
    "    top_grams = gram_sum.argsort()[::-1] \n",
    "     \n",
    "    return [gram_dict[i] for i in top_grams], gram_sum[top_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams, ngram_counts = {}, {} \n",
    "for n in [1, 2, 3]: \n",
    "    ngrams[n], ngram_counts[n] = \n",
    "get_top_grams([lemmatized_text], n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zipf \n",
    "def make_zipf_plot(counts, tokens, a=1.15): \n",
    "    ranks = np.arange(1, len(counts) + 1) \n",
    "    indices = np.argsort(-counts) \n",
    "    normalized_frequencies = counts[indices] / sum(counts) \n",
    "    f = plt.figure(figsize=(5.5, 5.5)) \n",
    "    plt.loglog(ranks, normalized_frequencies, marker=\".\") \n",
    "     \n",
    "    plt.loglog(ranks, [z for z in zipf.pmf(ranks, a)])\n",
    "    plt.title(\"Zipf Plot\") \n",
    "    plt.xlabel(\"Word frequency rank\") \n",
    "    plt.ylabel(\"Word frequency\") \n",
    "    ax = plt.gca() \n",
    "    ax.set_aspect('equal')  # make the plot square \n",
    "    plt.grid(True) \n",
    "     \n",
    "    # add text labels \n",
    "    last_freq = None \n",
    "    labeled_word_idxs = list(np.logspace(-0.5, \n",
    "                                         np.log10(len(counts) - 1), \n",
    "                                         10).astype(int)) \n",
    "    for i in labeled_word_idxs: \n",
    "        dummy = plt.text(ranks[i], \n",
    "                         normalized_frequencies[i], \n",
    "                         \" \" + tokens[indices[i]], \n",
    "                          verticalalignment=\"bottom\", \n",
    "                         horizontalalignment=\"left\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder \n",
    "BigramCollocationFinder.from_words(lemmatized_text.split()).nbest(BigramAssocMeasures().pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramCollocationFinder.from_words(lemmatized_text.split()).score_ngrams(BigramAssocMeasures().pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {} \n",
    "pos_dict['word'] = [] \n",
    "pos_dict['POS'] = [] \n",
    "for word in processed_text: \n",
    "    if word.is_alpha and not word.is_stop: \n",
    "        pos_dict['word'].append(word.lower_) \n",
    "        pos_dict['POS'].append(word.pos_) \n",
    "wnp_pos_df = pd.DataFrame(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = wnp_pos_df.groupby('POS').count().sort_values(by='word', ascending=False).head(10) \n",
    "pos_counts.columns = ['count'] \n",
    "wnp_pos_df['count'] = 1 \n",
    "wnp_pos_df.groupby(['POS', 'word']).count().sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(newsgroups_train['target_names'], \n",
    "range(len(newsgroups_train['target_names']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ng = ng_train_df[ng_train_df['label'] == \n",
    "14].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.nlp import setup, create_model, plot_model, assign_model \n",
    "space_setup = setup(space_ng, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = create_model('lda') \n",
    "plot_model(lda, 'topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = assign_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel \n",
    "cm = CoherenceModel(model=lda, \n",
    "texts=lda_results['text'].map(str.split).tolist(), dictionary=lda.id2word) \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [] \n",
    "for num_topics in range(2, 16): \n",
    "    lda = create_model('lda', num_topics=num_topics) \n",
    "    lda_results = assign_model(lda) \n",
    "    cm = CoherenceModel(model=lda,  \n",
    "texts=lda_results['text'].map(str.split).tolist(),  \n",
    "                        dictionary=lda.id2word) \n",
    "    coherences.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 16), coherences) \n",
    "plt.xlabel('number of LDA topics') \n",
    "plt.ylabel('coherence score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec \n",
    "raw_ng_df = pd.DataFrame({'text': newsgroups_train['data'],  'label': newsgroups_train['target']}) \n",
    "raw_space_df = raw_ng_df[raw_ng_df['label'] == 14] \n",
    "model = Top2Vec(documents=raw_space_df['text'].to_list(), \n",
    "workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(remove=('headers', \n",
    "'footers')) \n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  \n",
    "                                     remove=('headers', \n",
    "'footers')) \n",
    "en_large = spacy.load('en_core_web_lg', disable=['parser', \n",
    "'ner']) \n",
    "def get_document_vectors(text): \n",
    "    processed = en_large(text) \n",
    "    return processed.vector \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "                            'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_train_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") \n",
    "ng_test_df = pd.DataFrame({'text': newsgroups_test['data'], \n",
    "                           'label': \n",
    "newsgroups_test['target']}) \n",
    "ng_test_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_test_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_vector_df = pd.concat([ng_train_df['label'].astype('category'), \n",
    "                                ng_train_doc_vectors], axis=1) \n",
    "ng_test_vector_df = pd.concat([ng_test_df['label'].astype('category'), \n",
    "                               ng_test_doc_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "train_tfidf = vectorizer.fit_transform(newsgroups_train['data']) \n",
    "test_tfidf = vectorizer.transform(newsgroups_test['data']) \n",
    "train_tfidf_df = pd.DataFrame(train_tfidf.todense()) \n",
    "test_tfidf_df = pd.DataFrame(test_tfidf.todense()) \n",
    "train_tfidf_df['label'] = pd.Series(newsgroups_train['target']).astype('category') \n",
    "test_tfidf_df['label'] = pd.Series(newsgroups_test['target']).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression() \n",
    "lr.fit(ng_train_vector_df.drop('label', axis=1), \n",
    "ng_train_vector_df['label']) \n",
    "lr.score(ng_train_vector_df.drop('label', axis=1),  \n",
    "         ng_train_vector_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression() \n",
    "lr.fit(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label']) \n",
    "lr.score(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "vader = SentimentIntensityAnalyzer() \n",
    "def get_sentiment(text): \n",
    "    return vader.polarity_scores(text.lower())['compound'] \n",
    "ng_train_df['sentiment_score'] = ng_train_df['text'].swifter.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {i: label for i, label in \n",
    "              enumerate(newsgroups_train['target_names'])} \n",
    "ng_train_df['label'].replace(label_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df.groupby('label').mean().sort_values(by='sentiment_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df[(ng_train_df['label'] == 'talk.politics.guns') &  \n",
    "            (ng_train_df['sentiment_score'] < -0.5)].sample(3, random_state=42)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "guns_hockey_df = ng_train_df[ng_train_df['label'].isin(['talk.politics.guns', \n",
    "'rec.sport.hockey'])] \n",
    "sns.histplot(guns_hockey_df, \n",
    "             x='sentiment_score', \n",
    "             hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
