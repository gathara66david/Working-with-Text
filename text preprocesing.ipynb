{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gutenberg_cleaner\n",
      "  Downloading gutenberg_cleaner-0.1.6-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gutenberg_cleaner) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->gutenberg_cleaner) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->gutenberg_cleaner) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->gutenberg_cleaner) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->gutenberg_cleaner) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk->gutenberg_cleaner) (0.4.6)\n",
      "Downloading gutenberg_cleaner-0.1.6-py3-none-any.whl (7.4 kB)\n",
      "Installing collected packages: gutenberg_cleaner\n",
      "Successfully installed gutenberg_cleaner-0.1.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\David gathara marigi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\David gathara marigi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\David gathara marigi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install gutenberg_cleaner\n",
    "from urllib.request import urlopen \n",
    "from gutenberg_cleaner import simple_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp =   urlopen('http://www.gutenberg.org/cache/epub/10/pg10.txt').read().decode('utf-8')\n",
    "wnp = simple_cleaner(wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = wnp.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "wnp = re.sub(r'\\s+', ' ', wnp).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "wnp = wnp.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = re.sub(r'\\w+@\\w+\\.\\w+', '', wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words \n",
    "non_stopwords = [] \n",
    "for word in wnp.split(): \n",
    "if word not in _stop_words.ENGLISH_STOP_WORDS: \n",
    "        non_stopwords.append(word) \n",
    "cleaned_text = ' '.join(non_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.stem import SnowballStemmer \n",
    "stemmer = SnowballStemmer('english') \n",
    "stemmed_words = [] \n",
    "for word in cleaned_text.split(): \n",
    "    stemmed_words.append(stemmer.stem(word)) \n",
    "stemmed_text = ' '.join(stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_sm', disable=\n",
    " ['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "def clean_text_spacy(text): \n",
    "    processed_text = spacy_en_model(text) \n",
    "    lemmas = [w.lemma_ if w.lemma_ != '-PRON-' \n",
    "              else w.lower_ for w in processed_text \n",
    "              if w.is_alpha and not w.is_stop] \n",
    "    return ' '.join(lemmas).lower() \n",
    "wnp = urlopen('https://www.gutenberg.org/files/2600/2600\n",
    "0.txt').\\ \n",
    "    read().decode('utf-8') \n",
    "wnp = simple_cleaner(wnp) \n",
    "lemmatized_text = clean_text_spacy(wnp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en_model = spacy.load('en_core_web_sm') \n",
    "spacy_en_model.pipe_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_lg', disable=\n",
    " ['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "processed_text = spacy_en_model(wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in processed_text[:10]: \n",
    "    print(word.text, word.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups \n",
    "newsgroups_train = fetch_20newsgroups(remove=('headers', \n",
    "'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "ng_train_tfidf = \n",
    "tfidf_vectorizer.fit_transform(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import swifter \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_df['text'] = \n",
    "ng_train_df['text'].swifter.apply(clean_text_spacy) \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "ng_train_tfidf = tfidf_vectorizer.fit_transform \n",
    "(ng_train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.nlp import setup, plot_model \n",
    "nlp_setup = setup(newsgroups_train['data'], custom_stopwords=\n",
    " ['ax', 'edu', 'com', 'write']) \n",
    "plot_model(model=None, plot='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist \n",
    "fd = FreqDist(lemmatized_text.split()) \n",
    "fd.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    "fd_bg = FreqDist(map(' '.join, \n",
    "bigrams(lemmatized_text.split()))) \n",
    "fd_bg.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "idx_to_word = {v: k for k, v in \n",
    "tfidf_vectorizer.vocabulary_.items()} \n",
    "num_words = 20 \n",
    "tfidf_sum = np.asarray(ng_train_tfidf.sum(axis=0)).flatten() \n",
    "sorted_idx = tfidf_sum.argsort()[::-1] \n",
    "tfidf_sum = tfidf_sum[sorted_idx] \n",
    "xticks = range(num_words) \n",
    "plt.bar(xticks, tfidf_sum[:num_words]) \n",
    "plt.xticks(xticks, \n",
    "           [idx_to_word[i] for i in sorted_idx[:num_words]], \n",
    "           rotation=90) \n",
    "plt.xlabel('word') \n",
    "plt.ylabel('TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.histplot(ng_train_df['text'].apply(lambda x: len(x))) \n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer \n",
    "def get_top_grams(docs, n=2): \n",
    "    v = CountVectorizer(ngram_range=(n, n)) \n",
    "    grams = v.fit_transform(docs) \n",
    "    gram_sum = np.array(np.sum(grams, axis=0)).flatten() \n",
    "    gram_dict = {i: v for v, i in v.vocabulary_.items()} \n",
    "    top_grams = gram_sum.argsort()[::-1] \n",
    "     \n",
    "    return [gram_dict[i] for i in top_grams], \n",
    "gram_sum[top_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams, ngram_counts = {}, {} \n",
    "for n in [1, 2, 3]: \n",
    "    ngrams[n], ngram_counts[n] = \n",
    "get_top_grams([lemmatized_text], n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from scipy.stats import zipf \n",
    "def make_zipf_plot(counts, tokens, a=1.15): \n",
    "    ranks = np.arange(1, len(counts) + 1) \n",
    "    indices = np.argsort(-counts) \n",
    "    normalized_frequencies = counts[indices] / sum(counts) \n",
    "    f = plt.figure(figsize=(5.5, 5.5)) \n",
    "    plt.loglog(ranks, normalized_frequencies, marker=\".\") \n",
    "     \n",
    "    plt.loglog(ranks, [z for z in zipf.pmf(ranks, a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.title(\"Zipf Plot\") \n",
    "    plt.xlabel(\"Word frequency rank\") \n",
    "    plt.ylabel(\"Word frequency\") \n",
    "     \n",
    "    ax = plt.gca() \n",
    "    ax.set_aspect('equal')  # make the plot square \n",
    "    plt.grid(True) \n",
    "     \n",
    "    # add text labels \n",
    "    last_freq = None \n",
    "    labeled_word_idxs = list(np.logspace(-0.5, \n",
    "                                         np.log10(len(counts) - 1), \n",
    "                                         10).astype(int)) \n",
    "    for i in labeled_word_idxs: \n",
    "        dummy = plt.text(ranks[i], \n",
    "                         normalized_frequencies[i], \n",
    "                         \" \" + tokens[indices[i]], \n",
    "                          verticalalignment=\"bottom\", \n",
    "                         horizontalalignment=\"left\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.collocations import BigramAssocMeasures, \n",
    "BigramCollocationFinder \n",
    "BigramCollocationFinder.from_words(lemmatized_text.split()).\\ \n",
    "    nbest(BigramAssocMeasures().pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramCollocationFinder.from_words(lemmatized_text.split()).\\ \n",
    "    score_ngrams(BigramAssocMeasures().pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pos_dict = {} \n",
    "pos_dict['word'] = [] \n",
    "pos_dict['POS'] = [] \n",
    "for word in processed_text: \n",
    "    if word.is_alpha and not word.is_stop: \n",
    "        pos_dict['word'].append(word.lower_) pos_dict['POS'].append(word.pos_) \n",
    "wnp_pos_df = pd.DataFrame(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pos_counts = wnp_pos_df.groupby('POS').count().\\ \n",
    "                sort_values(by='word', \n",
    "ascending=False).head(10) \n",
    "pos_counts.columns = ['count'] \n",
    "wnp_pos_df['count'] = 1 \n",
    "wnp_pos_df.groupby(['POS', 'word']).count().\\ \n",
    "    sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(newsgroups_train['target_names'], \n",
    "range(len(newsgroups_train['target_names']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ng = ng_train_df[ng_train_df['label'] == \n",
    "14].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pycaret.nlp import setup, create_model, plot_model, \n",
    "assign_model \n",
    "space_setup = setup(space_ng, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " lda = create_model('lda') \n",
    "plot_model(lda, 'topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = assign_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel \n",
    "cm = CoherenceModel(model=lda, \n",
    "texts=lda_results['text'].map(str.split).tolist(), \n",
    "                    dictionary=lda.id2word) \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " coherences = [] \n",
    "for num_topics in range(2, 16): \n",
    "    lda = create_model('lda', num_topics=num_topics) \n",
    "    lda_results = assign_model(lda) \n",
    "    cm = CoherenceModel(model=lda,  \n",
    "texts=lda_results['text'].map(str.split).tolist(),  \n",
    "                        dictionary=lda.id2word) \n",
    "    coherences.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.plot(range(2, 16), coherences) \n",
    "plt.xlabel('number of LDA topics') \n",
    "plt.ylabel('coherence score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec \n",
    "raw_ng_df = pd.DataFrame({'text': newsgroups_train['data'],  \n",
    "'label': \n",
    "newsgroups_train['target']}) \n",
    "raw_space_df = raw_ng_df[raw_ng_df['label'] == 14] \n",
    "model = Top2Vec(documents=raw_space_df['text'].to_list(), \n",
    "workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(remove=('headers', \n",
    "'footers')) \n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  \n",
    "                                     remove=('headers', \n",
    "'footers')) \n",
    "en_large = spacy.load('en_core_web_lg', disable=['parser', \n",
    "'ner']) \n",
    "def get_document_vectors(text): \n",
    "    processed = en_large(text) \n",
    "    return processed.vector \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "                            'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_train_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") \n",
    "ng_test_df = pd.DataFrame({'text': newsgroups_test['data'], \n",
    "                           'label': \n",
    "newsgroups_test['target']}) \n",
    "ng_test_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_test_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_vector_df = \n",
    "pd.concat([ng_train_df['label'].astype('category'), \n",
    "                                ng_train_doc_vectors], axis=1) \n",
    "ng_test_vector_df = \n",
    "pd.concat([ng_test_df['label'].astype('category'), \n",
    "                               ng_test_doc_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "train_tfidf = \n",
    "vectorizer.fit_transform(newsgroups_train['data']) \n",
    "test_tfidf = vectorizer.transform(newsgroups_test['data']) \n",
    "train_tfidf_df = pd.DataFrame(train_tfidf.todense()) \n",
    "test_tfidf_df = pd.DataFrame(test_tfidf.todense()) \n",
    "train_tfidf_df['label'] = \n",
    "pd.Series(newsgroups_train['target']).\\ \n",
    "    astype('category') \n",
    "test_tfidf_df['label'] = \n",
    "pd.Series(newsgroups_test['target']).\\ \n",
    "    astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression() \n",
    "lr.fit(ng_train_vector_df.drop('label', axis=1), \n",
    "ng_train_vector_df['label']) \n",
    "lr.score(ng_train_vector_df.drop('label', axis=1),  \n",
    "         ng_train_vector_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " lr = LogisticRegression() \n",
    "lr.fit(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label']) \n",
    "lr.score(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import \n",
    "SentimentIntensityAnalyzer \n",
    "vader = SentimentIntensityAnalyzer() \n",
    "def get_sentiment(text): \n",
    "return vader.polarity_scores(text.lower())['compound'] \n",
    "ng_train_df['sentiment_score'] = ng_train_df['text'].\\ \n",
    "    swifter.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {i: label for i, label in \n",
    "              enumerate(newsgroups_train['target_names'])} \n",
    "ng_train_df['label'].replace(label_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df.groupby('label').mean().\\ \n",
    "    sort_values(by='sentiment_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df[(ng_train_df['label'] == 'talk.politics.guns') &  \n",
    "            (ng_train_df['sentiment_score'] < -0.5)].\\ \n",
    "    sample(3, random_state=42)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "guns_hockey_df = ng_train_df[ng_train_df['label']\\ \n",
    "                     .isin(['talk.politics.guns', \n",
    "'rec.sport.hockey'])] \n",
    "sns.histplot(guns_hockey_df, \n",
    "             x='sentiment_score', \n",
    "             hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
