{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gutenberg_cleaner\n",
      "  Using cached gutenberg_cleaner-0.1.6-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nltk (from gutenberg_cleaner)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->gutenberg_cleaner) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->gutenberg_cleaner) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk->gutenberg_cleaner)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->gutenberg_cleaner) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\david gathara marigi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk->gutenberg_cleaner) (0.4.6)\n",
      "Using cached gutenberg_cleaner-0.1.6-py3-none-any.whl (7.4 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Installing collected packages: regex, nltk, gutenberg_cleaner\n",
      "Successfully installed gutenberg_cleaner-0.1.6 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\David gathara marigi\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install gutenberg_cleaner\n",
    "from urllib.request import urlopen \n",
    "from gutenberg_cleaner import simple_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp =   urlopen('http://www.gutenberg.org/cache/epub/10/pg10.txt').read().decode('utf-8')\n",
    "wnp = simple_cleaner(wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = wnp.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "wnp = re.sub(r'\\s+', ' ', wnp).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "wnp = wnp.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnp = re.sub(r'\\w+@\\w+\\.\\w+', '', wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words \n",
    "non_stopwords = [] \n",
    "for word in wnp.split():  \n",
    "        if word not in _stop_words.ENGLISH_STOP_WORDS: \n",
    "                non_stopwords.append(word) \n",
    "cleaned_text = ' '.join(non_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\David gathara\n",
      "[nltk_data]     marigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer \n",
    "stemmer = SnowballStemmer('english') \n",
    "stemmed_words = [] \n",
    "for word in cleaned_text.split(): \n",
    "    stemmed_words.append(stemmer.stem(word)) \n",
    "stemmed_text = ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 11) (2592828154.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    wnp = urlopen('https://www.gutenberg.org/files/2600/2600\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 11)\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_sm', disable=\n",
    " ['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "def clean_text_spacy(text): \n",
    "    processed_text = spacy_en_model(text) \n",
    "    lemmas = [w.lemma_ if w.lemma_ != '-PRON-' \n",
    "              else w.lower_ for w in processed_text \n",
    "              if w.is_alpha and not w.is_stop] \n",
    "    return ' '.join(lemmas).lower() \n",
    "wnp = urlopen('https://www.gutenberg.org/files/2600/2600\n",
    "0.txt').\\ \n",
    "    read().decode('utf-8') \n",
    "wnp = simple_cleaner(wnp) \n",
    "lemmatized_text = clean_text_spacy(wnp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en_model = spacy.load('en_core_web_sm') \n",
    "spacy_en_model.pipe_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "spacy_en_model = spacy.load('en_core_web_lg', disable=\n",
    " ['parser', 'ner']) \n",
    "spacy_en_model.max_length = 4000000 \n",
    "processed_text = spacy_en_model(wnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in processed_text[:10]: \n",
    "    print(word.text, word.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups \n",
    "newsgroups_train = fetch_20newsgroups(remove=('headers', \n",
    "'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "ng_train_tfidf = \n",
    "tfidf_vectorizer.fit_transform(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import swifter \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_df['text'] = \n",
    "ng_train_df['text'].swifter.apply(clean_text_spacy) \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "ng_train_tfidf = tfidf_vectorizer.fit_transform \n",
    "(ng_train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.nlp import setup, plot_model \n",
    "nlp_setup = setup(newsgroups_train['data'], custom_stopwords=\n",
    " ['ax', 'edu', 'com', 'write']) \n",
    "plot_model(model=None, plot='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist \n",
    "fd = FreqDist(lemmatized_text.split()) \n",
    "fd.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    "fd_bg = FreqDist(map(' '.join, \n",
    "bigrams(lemmatized_text.split()))) \n",
    "fd_bg.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "idx_to_word = {v: k for k, v in \n",
    "tfidf_vectorizer.vocabulary_.items()} \n",
    "num_words = 20 \n",
    "tfidf_sum = np.asarray(ng_train_tfidf.sum(axis=0)).flatten() \n",
    "sorted_idx = tfidf_sum.argsort()[::-1] \n",
    "tfidf_sum = tfidf_sum[sorted_idx] \n",
    "xticks = range(num_words) \n",
    "plt.bar(xticks, tfidf_sum[:num_words]) \n",
    "plt.xticks(xticks, \n",
    "           [idx_to_word[i] for i in sorted_idx[:num_words]], \n",
    "           rotation=90) \n",
    "plt.xlabel('word') \n",
    "plt.ylabel('TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.histplot(ng_train_df['text'].apply(lambda x: len(x))) \n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer \n",
    "def get_top_grams(docs, n=2): \n",
    "    v = CountVectorizer(ngram_range=(n, n)) \n",
    "    grams = v.fit_transform(docs) \n",
    "    gram_sum = np.array(np.sum(grams, axis=0)).flatten() \n",
    "    gram_dict = {i: v for v, i in v.vocabulary_.items()} \n",
    "    top_grams = gram_sum.argsort()[::-1] \n",
    "     \n",
    "    return [gram_dict[i] for i in top_grams], \n",
    "gram_sum[top_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams, ngram_counts = {}, {} \n",
    "for n in [1, 2, 3]: \n",
    "    ngrams[n], ngram_counts[n] = \n",
    "get_top_grams([lemmatized_text], n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from scipy.stats import zipf \n",
    "def make_zipf_plot(counts, tokens, a=1.15): \n",
    "    ranks = np.arange(1, len(counts) + 1) \n",
    "    indices = np.argsort(-counts) \n",
    "    normalized_frequencies = counts[indices] / sum(counts) \n",
    "    f = plt.figure(figsize=(5.5, 5.5)) \n",
    "    plt.loglog(ranks, normalized_frequencies, marker=\".\") \n",
    "     \n",
    "    plt.loglog(ranks, [z for z in zipf.pmf(ranks, a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.title(\"Zipf Plot\") \n",
    "    plt.xlabel(\"Word frequency rank\") \n",
    "    plt.ylabel(\"Word frequency\") \n",
    "     \n",
    "    ax = plt.gca() \n",
    "    ax.set_aspect('equal')  # make the plot square \n",
    "    plt.grid(True) \n",
    "     \n",
    "    # add text labels \n",
    "    last_freq = None \n",
    "    labeled_word_idxs = list(np.logspace(-0.5, \n",
    "                                         np.log10(len(counts) - 1), \n",
    "                                         10).astype(int)) \n",
    "    for i in labeled_word_idxs: \n",
    "        dummy = plt.text(ranks[i], \n",
    "                         normalized_frequencies[i], \n",
    "                         \" \" + tokens[indices[i]], \n",
    "                          verticalalignment=\"bottom\", \n",
    "                         horizontalalignment=\"left\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.collocations import BigramAssocMeasures, \n",
    "BigramCollocationFinder \n",
    "BigramCollocationFinder.from_words(lemmatized_text.split()).\\ \n",
    "    nbest(BigramAssocMeasures().pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigramCollocationFinder.from_words(lemmatized_text.split()).\\ \n",
    "    score_ngrams(BigramAssocMeasures().pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pos_dict = {} \n",
    "pos_dict['word'] = [] \n",
    "pos_dict['POS'] = [] \n",
    "for word in processed_text: \n",
    "    if word.is_alpha and not word.is_stop: \n",
    "        pos_dict['word'].append(word.lower_) pos_dict['POS'].append(word.pos_) \n",
    "wnp_pos_df = pd.DataFrame(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pos_counts = wnp_pos_df.groupby('POS').count().\\ \n",
    "                sort_values(by='word', \n",
    "ascending=False).head(10) \n",
    "pos_counts.columns = ['count'] \n",
    "wnp_pos_df['count'] = 1 \n",
    "wnp_pos_df.groupby(['POS', 'word']).count().\\ \n",
    "    sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(newsgroups_train['target_names'], \n",
    "range(len(newsgroups_train['target_names']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ng = ng_train_df[ng_train_df['label'] == \n",
    "14].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pycaret.nlp import setup, create_model, plot_model, \n",
    "assign_model \n",
    "space_setup = setup(space_ng, target='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " lda = create_model('lda') \n",
    "plot_model(lda, 'topic_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = assign_model(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel \n",
    "cm = CoherenceModel(model=lda, \n",
    "texts=lda_results['text'].map(str.split).tolist(), \n",
    "                    dictionary=lda.id2word) \n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " coherences = [] \n",
    "for num_topics in range(2, 16): \n",
    "    lda = create_model('lda', num_topics=num_topics) \n",
    "    lda_results = assign_model(lda) \n",
    "    cm = CoherenceModel(model=lda,  \n",
    "texts=lda_results['text'].map(str.split).tolist(),  \n",
    "                        dictionary=lda.id2word) \n",
    "    coherences.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.plot(range(2, 16), coherences) \n",
    "plt.xlabel('number of LDA topics') \n",
    "plt.ylabel('coherence score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec \n",
    "raw_ng_df = pd.DataFrame({'text': newsgroups_train['data'],  \n",
    "'label': \n",
    "newsgroups_train['target']}) \n",
    "raw_space_df = raw_ng_df[raw_ng_df['label'] == 14] \n",
    "model = Top2Vec(documents=raw_space_df['text'].to_list(), \n",
    "workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(remove=('headers', \n",
    "'footers')) \n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  \n",
    "                                     remove=('headers', \n",
    "'footers')) \n",
    "en_large = spacy.load('en_core_web_lg', disable=['parser', \n",
    "'ner']) \n",
    "def get_document_vectors(text): \n",
    "    processed = en_large(text) \n",
    "    return processed.vector \n",
    "ng_train_df = pd.DataFrame({'text': newsgroups_train['data'], \n",
    "                            'label': \n",
    "newsgroups_train['target']}) \n",
    "ng_train_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_train_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") \n",
    "ng_test_df = pd.DataFrame({'text': newsgroups_test['data'], \n",
    "                           'label': \n",
    "newsgroups_test['target']}) \n",
    "ng_test_doc_vectors = pd.DataFrame( \n",
    "    np.vstack(ng_test_df['text']. \n",
    "              swifter.apply(get_document_vectors).tolist()) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_vector_df = \n",
    "pd.concat([ng_train_df['label'].astype('category'), \n",
    "                                ng_train_doc_vectors], axis=1) \n",
    "ng_test_vector_df = \n",
    "pd.concat([ng_test_df['label'].astype('category'), \n",
    "                               ng_test_doc_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " vectorizer = TfidfVectorizer(min_df=10, max_df=0.9) \n",
    "train_tfidf = \n",
    "vectorizer.fit_transform(newsgroups_train['data']) \n",
    "test_tfidf = vectorizer.transform(newsgroups_test['data']) \n",
    "train_tfidf_df = pd.DataFrame(train_tfidf.todense()) \n",
    "test_tfidf_df = pd.DataFrame(test_tfidf.todense()) \n",
    "train_tfidf_df['label'] = \n",
    "pd.Series(newsgroups_train['target']).\\ \n",
    "    astype('category') \n",
    "test_tfidf_df['label'] = \n",
    "pd.Series(newsgroups_test['target']).\\ \n",
    "    astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression() \n",
    "lr.fit(ng_train_vector_df.drop('label', axis=1), \n",
    "ng_train_vector_df['label']) \n",
    "lr.score(ng_train_vector_df.drop('label', axis=1),  \n",
    "         ng_train_vector_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " lr = LogisticRegression() \n",
    "lr.fit(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label']) \n",
    "lr.score(train_tfidf_df.drop('label', axis=1), \n",
    "train_tfidf_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import \n",
    "SentimentIntensityAnalyzer \n",
    "vader = SentimentIntensityAnalyzer() \n",
    "def get_sentiment(text): \n",
    "return vader.polarity_scores(text.lower())['compound'] \n",
    "ng_train_df['sentiment_score'] = ng_train_df['text'].\\ \n",
    "    swifter.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {i: label for i, label in \n",
    "              enumerate(newsgroups_train['target_names'])} \n",
    "ng_train_df['label'].replace(label_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df.groupby('label').mean().\\ \n",
    "    sort_values(by='sentiment_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_df[(ng_train_df['label'] == 'talk.politics.guns') &  \n",
    "            (ng_train_df['sentiment_score'] < -0.5)].\\ \n",
    "    sample(3, random_state=42)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "guns_hockey_df = ng_train_df[ng_train_df['label']\\ \n",
    "                     .isin(['talk.politics.guns', \n",
    "'rec.sport.hockey'])] \n",
    "sns.histplot(guns_hockey_df, \n",
    "             x='sentiment_score', \n",
    "             hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
